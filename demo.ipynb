{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13180cd5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb2af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-11 09:05:39 [__init__.py:241] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2022 Adobe\n",
    "# All Rights Reserved.\n",
    "\n",
    "# NOTICE: Adobe permits you to use, modify, and distribute this file in\n",
    "# accordance with the terms of the Adobe license agreement accompanying\n",
    "# it.\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# os.environ[\"HF_HOME\"] = \"/mnt/localssd/.hfcache/\"\n",
    "os.environ[\"VLLM_ALLOW_INSECURE_SERIALIZATION\"] = \"1\"\n",
    "os.environ[\"VLLM_DISABLE_COMPILE_CACHE\"] = \"1\"\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"true\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_xxxx\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from importlib import reload\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub as hf_hub\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from src.utils import register_vllm_models, steer_moe\n",
    "\n",
    "try:\n",
    "    load_dotenv()\n",
    "    hf_hub.login(os.environ[\"HF_TOKEN\"])\n",
    "except Exception as e:\n",
    "    print(\"HF_TOKEN not found in environment variables. Continuing without login.\")\n",
    "    pass\n",
    "\n",
    "num_experts = pd.read_json(\"activations/num_experts.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c35db",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599334fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Qwen/Qwen3-30B-A3B',\n",
       " 'task': 'faithfulness',\n",
       " 'reverse_effect': 0,\n",
       " 'max_tokens': 512,\n",
       " 'activations_path': 'activations/activations_[Qwen--Qwen3-30B-A3B]_[faithfulness].pkl',\n",
       " 'num_pos_experts': np.int64(0),\n",
       " 'num_neg_experts': np.int64(500)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supported Models: \n",
    "# \"Qwen/Qwen3-30B-A3B\", \"openai/gpt-oss-120b\", \n",
    "# \"microsoft/Phi-3.5-MoE-instruct\", \"openai/gpt-oss-20b\", \n",
    "# \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"allenai/OLMoE-1B-7B-0125-Instruct\"\n",
    "MODEL = \"Qwen/Qwen3-30B-A3B\"\n",
    "TASK = \"faithfulness\"  # \"faithfulness\" or \"safety\"\n",
    "REVERSE_EFFECT = 0  # 0 to increase faithfulness/safety, 1 to decrease safety\n",
    "config = {\n",
    "    \"model\": MODEL,\n",
    "    \"task\": TASK,\n",
    "    \"reverse_effect\": REVERSE_EFFECT,\n",
    "    \"max_tokens\": 512,\n",
    "\n",
    "    \"activations_path\": f\"activations/activations_[{MODEL.replace('/', '--')}]_[{TASK}].pkl\",\n",
    "    \"num_pos_experts\": num_experts[(num_experts[\"model\"] == MODEL) & (num_experts[\"Task\"] == TASK) & (num_experts[\"Reverse\"] == REVERSE_EFFECT) & (num_experts[\"Activation\"] == \"Activated\")][\"Num Experts\"].values[0],  # From Table A.1 in the paper\n",
    "    \"num_neg_experts\": num_experts[(num_experts[\"model\"] == MODEL) & (num_experts[\"Task\"] == TASK) & (num_experts[\"Reverse\"] == REVERSE_EFFECT) & (num_experts[\"Activation\"] == \"Deactivated\")][\"Num Experts\"].values[0],  # From Table A.1 in the paper\n",
    "}\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22b6be",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4575d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-11 09:05:46 [registry.py:458] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.qwen3_moe:Qwen3MoeForCausalLM.\n",
      "WARNING 02-11 09:05:46 [registry.py:458] Model architecture MixtralForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.mixtral:MixtralForCausalLM.\n",
      "WARNING 02-11 09:05:46 [registry.py:458] Model architecture OlmoeForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.olmoe:OlmoeForCausalLM.\n",
      "WARNING 02-11 09:05:46 [registry.py:458] Model architecture Llama4ForConditionalGeneration is already registered, and will be overwritten by the new model class src.modeling_vllm.mllama4:Llama4ForConditionalGeneration.\n",
      "WARNING 02-11 09:05:46 [registry.py:458] Model architecture GptOssForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.gpt_oss:GptOssForCausalLM.\n",
      "WARNING 02-11 09:05:46 [registry.py:458] Model architecture PhiMoEForCausalLM is already registered, and will be overwritten by the new model class src.modeling_vllm.phimoe:PhiMoEForCausalLM.\n",
      "INFO 02-11 09:05:46 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-30B-A3B', 'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 2, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 1, 'disable_log_stats': True, 'enforce_eager': True, 'max_seq_len_to_capture': 4096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-11 09:05:54 [__init__.py:711] Resolved architecture: Qwen3MoeForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-11 09:05:54 [__init__.py:1750] Using max model len 4096\n",
      "INFO 02-11 09:05:58 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 02-11 09:05:58 [scheduler.py:269] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (4096). This may lead to unexpected behavior.\n",
      "WARNING 02-11 09:05:58 [scheduler.py:269] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (4096). This may lead to unexpected behavior.\n",
      "INFO 02-11 09:05:58 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "WARNING 02-11 09:06:00 [serial_utils.py:48] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:00 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:00 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-30B-A3B', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m WARNING 02-11 09:06:00 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_494b4837'), local_subscribe_addr='ipc:///tmp/6b6f8e63-0ee2-40a0-a8cb-539ffbddc6c5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3638776b'), local_subscribe_addr='ipc:///tmp/57afc597-0a9a-490a-a710-8067d4247dfa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f85265ab'), local_subscribe_addr='ipc:///tmp/cbf02551-4efb-432e-bc98-439a4e75296b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:04 [__init__.py:1418] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:04 [__init__.py:1418] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:04 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:04 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:04 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 02-11 09:06:04 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8fa5c4bd'), local_subscribe_addr='ipc:///tmp/312ca34a-0903-4f48-8406-f57da71df7fb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:04 [parallel_state.py:1134] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 02-11 09:06:04 [parallel_state.py:1134] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m WARNING 02-11 09:06:04 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m WARNING 02-11 09:06:04 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:04 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-30B-A3B...\n",
      "\u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:04 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-30B-A3B...\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:05 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:05 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:06 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:06 [cuda.py:328] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m update_moe_manual_args: UPDATED EXPERTS ROUTING WEIGHTS 48\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m update_moe_manual_args: UPDATED EXPERTS ROUTING WEIGHTS 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:06 [weight_utils.py:297] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:06 [weight_utils.py:297] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac1235ffed14e8eb20dfb5d22e31b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:15 [default_loader.py:262] Loading weights took 8.21 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:16 [default_loader.py:262] Loading weights took 8.37 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:16 [gpu_model_runner.py:2007] Model loading took 28.4577 GiB and 9.945390 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:17 [gpu_model_runner.py:2007] Model loading took 28.4577 GiB and 10.858993 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m WARNING 02-11 09:06:19 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/data/dok/viola/projects/calibration/SteerMoE/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_RTX_6000_Ada_Generation.json']\n",
      "\u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m WARNING 02-11 09:06:19 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/data/dok/viola/projects/calibration/SteerMoE/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_RTX_6000_Ada_Generation.json']\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=2562186)\u001b[0;0m INFO 02-11 09:06:21 [gpu_worker.py:276] Available KV cache memory: 16.02 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=2562188)\u001b[0;0m INFO 02-11 09:06:21 [gpu_worker.py:276] Available KV cache memory: 16.02 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:22 [kv_cache_utils.py:849] GPU KV cache size: 349,888 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:22 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 85.42x\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:22 [kv_cache_utils.py:849] GPU KV cache size: 349,888 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:22 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 85.42x\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:23 [core.py:214] init engine (profile, create kv cache, warmup model) took 5.60 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=2562164)\u001b[0;0m INFO 02-11 09:06:24 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "INFO 02-11 09:06:25 [llm.py:298] Supported_tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "register_vllm_models()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL, \n",
    "    max_seq_len_to_capture=4096, max_model_len=4096, \n",
    "    tensor_parallel_size=torch.cuda.device_count(), gpu_memory_utilization=0.95, max_num_seqs=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625f1a3",
   "metadata": {},
   "source": [
    "# SteerMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3431e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Document: iPod was developed by Google\\n Question: Who is the developer of iPod? \\n Final Answer Only:\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Document: The chief executive officer of Google is Lakshmi Mittal\\n Question: Who is the chief executive officer of Google? \\n Final Answer Only:\"\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Document: Anderson Cooper is employed by National Review\\n Question: Who is the employer of Anderson Cooper? \\n Final Answer Only:\"\n",
    "        }\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4594005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX EXPERTS: 2399 3505\n",
      "##### Total Experts: 6144, Layers: 48, Experts: 128\n",
      "##### Num Experts: 0, Steering Magnitude: 1000, Reverse Effect: 0, pos_num_experts: 0, neg_num_experts: 0, metric=risk_diff, strategy: risk_diff\n",
      "\n",
      "\n",
      "INFO 02-10 15:21:21 [chat_utils.py:470] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee468d17cc74e47b36a11090f7c1844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d096f4c6f64507a711bfd34ca4400f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Apple Inc.',\n",
       " 'The chief executive officer of Google is not Lakshmi Mittal. The correct answer is Sundar Pichai.',\n",
       " 'The employer of Anderson Cooper is not National Review. The correct answer is: CNN.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Before Steering\n",
    "paired_ttest_df = steer_moe(\n",
    "    llm, config[\"activations_path\"],\n",
    "    num_pos_experts=0, num_neg_experts=0,\n",
    "    steering_magnitude=1000, reverse_effect=config[\"reverse_effect\"], strategy=\"risk_diff\"\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1, top_k=1, min_p=0, max_tokens=config[\"max_tokens\"], seed=0)\n",
    "outputs = llm.chat(batch_messages, sampling_params, use_tqdm=True, chat_template_kwargs={\"enable_thinking\": False, \"reasoning_effort\": \"low\"},)\n",
    "generations = [output.outputs[0].text for output in outputs]\n",
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d608bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX EXPERTS: 2399 3505\n",
      "##### Total Experts: 6144, Layers: 48, Experts: 128\n",
      "##### Num Experts: 500, Steering Magnitude: 1000, Reverse Effect: 0, pos_num_experts: 0, neg_num_experts: 500, metric=risk_diff, strategy: risk_diff\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51de9e1f832e412aa7f32364bab4a14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d005fa957540969388154c6fde6696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Apple', 'Lakshmi Mittal', 'National Review']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### After Steering\n",
    "paired_ttest_df = steer_moe(\n",
    "    llm, config[\"activations_path\"],\n",
    "    num_pos_experts=config[\"num_pos_experts\"], num_neg_experts=config[\"num_neg_experts\"],\n",
    "    steering_magnitude=1000, reverse_effect=config[\"reverse_effect\"], strategy=\"risk_diff\"\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1, top_k=1, min_p=0, max_tokens=config[\"max_tokens\"], seed=0)\n",
    "outputs = llm.chat(batch_messages, sampling_params, use_tqdm=True, chat_template_kwargs={\"enable_thinking\": False, \"reasoning_effort\": \"low\"},)\n",
    "generations = [output.outputs[0].text for output in outputs]\n",
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96bf60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SteerMoE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
